{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Import library"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-21T08:29:39.402643Z","iopub.status.busy":"2023-10-21T08:29:39.401975Z","iopub.status.idle":"2023-10-21T08:29:39.409638Z","shell.execute_reply":"2023-10-21T08:29:39.408630Z","shell.execute_reply.started":"2023-10-21T08:29:39.402613Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import evaluate\n","import pickle\n","import random\n","import tqdm\n","import matplotlib.pyplot as plt\n","from decimal import Decimal, getcontext\n","getcontext().prec = 64\n","import warnings\n","warnings.filterwarnings('ignore')\n","from scipy.special import rel_entr\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorForTokenClassification\n","from transformers import AutoModelForTokenClassification\n","from transformers import Trainer\n","from transformers import TrainingArguments\n","from transformers import get_scheduler\n","from huggingface_hub import Repository, get_full_repo_name\n","from accelerate import Accelerator\n","from tqdm.auto import tqdm\n","from datasets import *"]},{"cell_type":"markdown","metadata":{},"source":["## Create NER labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-21T07:56:46.078523Z","iopub.status.busy":"2023-10-21T07:56:46.078109Z","iopub.status.idle":"2023-10-21T07:56:46.087398Z","shell.execute_reply":"2023-10-21T07:56:46.086296Z","shell.execute_reply.started":"2023-10-21T07:56:46.078485Z"},"trusted":true},"outputs":[],"source":["entity = ['PATIENT'   , 'DOCTOR'       , 'USERNAME'  ,\n","          'PROFESSION',\n","          'ROOM'      , 'DEPARTMENT'   , 'HOSPITAL'  , 'ORGANIZATION', 'STREET' , 'CITY'    , 'STATE' , 'COUNTRY', 'ZIP'  , 'LOCATION-OTHER', \n","          'AGE'       , \n","          'DATE'      , 'TIME'         , 'DURATION'  , 'SET'         , \n","          'PHONE'     , 'FAX'          , 'EMAIL'     , 'URL'         , 'IPADDR' , \n","          'SSN'       , 'MEDICALRECORD', 'HEALTHPLAN', 'ACCOUNT'     , 'LICENSE', 'VECHICLE', 'DEVICE', 'BIOID'  , 'IDNUM']\n","label_names = ['OTHER']\n","entity_names = []\n","entity_count = [0] * len(entity)\n","\n","for s in entity:\n","    label_names.append(f'B-{s}')\n","    label_names.append(f'I-{s}')\n","    entity_names.append(s)\n","    \n","id2label = {i: label for i, label in enumerate(label_names)}\n","label2id = {v: k for k, v in id2label.items()}\n","org_id2label = {i: label for i, label in enumerate(entity_names)}\n","org_label2id = {v: k for k, v in org_id2label.items()}"]},{"cell_type":"markdown","metadata":{},"source":["## Create dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-21T07:56:46.090264Z","iopub.status.busy":"2023-10-21T07:56:46.089981Z","iopub.status.idle":"2023-10-21T07:56:46.105803Z","shell.execute_reply":"2023-10-21T07:56:46.104995Z","shell.execute_reply.started":"2023-10-21T07:56:46.090239Z"},"trusted":true},"outputs":[],"source":["def Spilt2Words(name, f, fa):\n","    tok = []\n","    ner = []\n","    lidx = 0\n","    ridx = 0\n","    while True:\n","        # remove last '\\n'\n","        ans_info = fa.readline()[:-1].split('\\t')\n","        # remove normalized DATE/TIME\n","        if (ans_info[1] == 'DATE' or ans_info[1] == 'TIME'): ans_info = ans_info[:-1]\n","            \n","        if (ans_info[1] != 'OTHER'): entity_count[org_label2id[ans_info[1]]] += 1\n","            \n","        ent_lidx, ent_ridx = int(ans_info[2]), int(ans_info[3])\n","\n","        # find next ans_info\n","        while True:\n","            word = ''\n","            # find next word lidx\n","            while True:\n","                nxt_char = f.read(1)\n","                if (nxt_char == ' ' or nxt_char == '\\n' or nxt_char == '\\t'): \n","                    lidx += 1\n","                else: \n","                    word += nxt_char\n","                    break\n","            ridx = lidx\n","            # find next word ridx\n","            while True:\n","                char_pos = f.tell()\n","                nxt_char = f.read(1)\n","                if (nxt_char == ' ' or nxt_char == '\\n' or nxt_char == '\\t' or ridx + 1 == ent_ridx):\n","                    ridx += 1\n","                    f.seek(char_pos)\n","                    break\n","                else:\n","                    ridx += 1\n","                    word += nxt_char\n","                \n","            line_end = 0\n","            # remove '\\n' in last word\n","            if (word[:-1] == '\\n'): \n","                line_end = 1\n","                word = word[:-1]\n","            # truncate beginning of the word if it is an entity word\n","            while (lidx < ent_lidx and ridx > ent_lidx and ridx <= ent_ridx):\n","                lidx += 1\n","                word = word[1:]\n","                \n","            tok.append(word)\n","            \n","            if (lidx < ent_lidx):\n","                ner.append(label2id['OTHER'])\n","            elif (lidx == ent_lidx):\n","                ner.append(label2id['B-' + ans_info[1]])\n","            elif (ridx <= ent_ridx):\n","                ner.append(label2id['I-' + ans_info[1]])\n","            \n","            lidx = ridx\n","            \n","            if (ridx == ent_ridx): # found the last word of entity, move to next answer info\n","                break\n","        \n","        info_pos = fa.tell()\n","        nxt_info = fa.readline()[:-1].split('\\t')\n","        fa.seek(info_pos)\n","        # nxt_info is in next file\n","        if (nxt_info[0] != name): \n","            break\n","        # nxt_info is in current file but has overlap in current info\n","        if (int(nxt_info[3]) <= ent_ridx):\n","            nxt_info = fa.readline()\n","            \n","    return tok, ner"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-21T07:56:46.107154Z","iopub.status.busy":"2023-10-21T07:56:46.106802Z","iopub.status.idle":"2023-10-21T07:56:46.118415Z","shell.execute_reply":"2023-10-21T07:56:46.117520Z","shell.execute_reply.started":"2023-10-21T07:56:46.107121Z"},"trusted":true},"outputs":[],"source":["def Segmentation(ds_id, ds_tok, ds_ner, id, tok, ner, l):\n","    while (len(ner) >= l):\n","        ridx = l\n","        k = random.randint(0, 1)\n","        while (ridx > 0 and ridx < len(ner) and id2label[ner[ridx]] != 'OTHER'):\n","            if (k): \n","                ridx += 1\n","            else:\n","                ridx -= 1\n","        if (ridx == 0):\n","            ridx = len(ner)\n","        elif (ridx < len(ner)):\n","            ridx += 1\n","        ds_id.append(id)\n","        ds_tok.append(tok[:ridx])\n","        ds_ner.append(ner[:ridx])\n","        tok = tok[ridx:]\n","        ner = ner[ridx:]\n","    if (len(ner) > 0):\n","        ds_id.append(id)\n","        ds_tok.append(tok)\n","        ds_ner.append(ner)\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-21T07:56:46.120381Z","iopub.status.busy":"2023-10-21T07:56:46.119631Z","iopub.status.idle":"2023-10-21T07:57:16.071974Z","shell.execute_reply":"2023-10-21T07:57:16.071032Z","shell.execute_reply.started":"2023-10-21T07:56:46.120349Z"},"trusted":true},"outputs":[],"source":["ds_dict = {'id':[], 'tokens':[], 'ner_tags':[]}\n","\n","fnames = [f for f in os.listdir('./First_Phase_Release(Correction)/First_Phase_Text_Dataset')]\n","fnames.sort()\n","\n","max_word_length = 80\n","fa = open('./First_Phase_Release(Correction)/answer.txt', 'r')\n","for fname in tqdm(fnames):\n","    f = open(f'./First_Phase_Release(Correction)/First_Phase_Text_Dataset/{fname}', 'r')\n","    tok, ner = Spilt2Words(fname[:-4], f, fa)\n","    if (max_word_length > 0):\n","        Segmentation(ds_dict['id'], ds_dict['tokens'], ds_dict['ner_tags'], fname[:-4], tok, ner, max_word_length)\n","    else:\n","        ds_dict['id'].append(fname[:-4])\n","        ds_dict['tokens'].append(tok)\n","        ds_dict['ner_tags'].append(ner)\n","    f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fnames = [f for f in os.listdir('./Second_Phase_Dataset/Second_Phase_Text_Dataset')]\n","fnames.sort()\n","\n","max_word_length = 80\n","fa = open('./Second_Phase_Dataset/answer.txt', 'r')\n","for fname in tqdm(fnames):\n","    f = open(f'./Second_Phase_Dataset/Second_Phase_Text_Dataset/{fname}', 'r')\n","    tok, ner = Spilt2Words(fname[:-4], f, fa)\n","    if (max_word_length > 0):\n","        Segmentation(ds_dict['id'], ds_dict['tokens'], ds_dict['ner_tags'], fname[:-4], tok, ner, max_word_length)\n","    else:\n","        ds_dict['id'].append(fname[:-4])\n","        ds_dict['tokens'].append(tok)\n","        ds_dict['ner_tags'].append(ner)\n","    f.close()"]},{"cell_type":"markdown","metadata":{},"source":["## Spilt train & dev data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-21T08:42:28.363269Z","iopub.status.busy":"2023-10-21T08:42:28.362285Z","iopub.status.idle":"2023-10-21T08:42:28.371023Z","shell.execute_reply":"2023-10-21T08:42:28.370037Z","shell.execute_reply.started":"2023-10-21T08:42:28.363233Z"},"trusted":true},"outputs":[],"source":["def CountSim(train, valid):\n","    tcnt = [0] * len(entity)\n","    vcnt = [0] * len(entity)\n","    for tdata in train:\n","        for t in tdata:\n","            if (t != 0 and id2label[t][0] != 'I'): tcnt[org_label2id[id2label[t][2:]]] += 1\n","    for vdata in valid:\n","        for v in vdata:\n","            if (v != 0 and id2label[v][0] != 'I'): vcnt[org_label2id[id2label[v][2:]]] += 1\n","    tsum = sum(tcnt)\n","    vsum = sum(vcnt)\n","    dist = 0\n","    for i in range(len(entity)):\n","        tcnt[i] = tcnt[i]/tsum\n","        vcnt[i] = vcnt[i]/vsum\n","        dist += abs(tcnt[i] - vcnt[i]) * abs(tcnt[i] - vcnt[i])\n","    return tcnt, vcnt, dist"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-21T08:42:36.652326Z","iopub.status.busy":"2023-10-21T08:42:36.651970Z","iopub.status.idle":"2023-10-21T08:54:22.570937Z","shell.execute_reply":"2023-10-21T08:54:22.569975Z","shell.execute_reply.started":"2023-10-21T08:42:36.652297Z"},"trusted":true},"outputs":[],"source":["best_ds_train_valid = Dataset.from_dict(ds_dict).train_test_split(train_size=0.9)\n","best_tpor = [0] * len(entity)\n","best_vpor = [0] * len(entity)\n","best_dist = 1\n","upper_bound = 2e-5\n","try_step = 1000\n","while (best_dist > upper_bound):\n","    for i in tqdm(range(try_step)):\n","        cur_ds_train_valid = Dataset.from_dict(ds_dict).train_test_split(train_size=0.8)\n","        cur_tpor, cur_vpor, cur_dist = CountSim(cur_ds_train_valid['train']['ner_tags'], cur_ds_train_valid['test']['ner_tags'])\n","        if (cur_dist < best_dist):\n","            best_ds_train_valid = cur_ds_train_valid\n","            best_tpor = cur_tpor\n","            best_vpor = cur_vpor\n","            best_dist = cur_dist\n","            print(f'New smallest dist = {best_dist}')\n","    \n","x = np.arange(len(entity_names))\n","width = 0.4\n","plt.figure(figsize=(12.8, 4.8))\n","plt.bar(x, best_tpor, width, color='green', label='Train')\n","plt.bar(x + width, best_vpor, width, color='blue', label='Dev')\n","plt.xticks(x + width / 2, entity_names, rotation='vertical')\n","plt.ylabel('Porpotion')\n","plt.title('TrainDev distribution')\n","plt.legend()\n","plt.savefig('TrainDev distribution')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.597828Z","iopub.status.idle":"2023-10-20T17:35:45.598264Z","shell.execute_reply":"2023-10-20T17:35:45.598122Z","shell.execute_reply.started":"2023-10-20T17:35:45.598103Z"},"trusted":true},"outputs":[],"source":["raw_ds = DatasetDict({'train': best_ds_train_valid['train'],\n","                  'validation': best_ds_train_valid['test']})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# raw_ds = load_from_disk(\"./ner_dataset/\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["raw_ds"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.601639Z","iopub.status.idle":"2023-10-20T17:35:45.602069Z","shell.execute_reply":"2023-10-20T17:35:45.601876Z","shell.execute_reply.started":"2023-10-20T17:35:45.601836Z"},"trusted":true},"outputs":[],"source":["model_name = \"hfl/english-pert-large\"\n","\n","model_checkpoint = model_name\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.605394Z","iopub.status.idle":"2023-10-20T17:35:45.605839Z","shell.execute_reply":"2023-10-20T17:35:45.605636Z","shell.execute_reply.started":"2023-10-20T17:35:45.605616Z"},"trusted":true},"outputs":[],"source":["def align_labels_with_tokens(labels, word_ids):\n","    new_labels = []\n","    current_word = None\n","    for word_id in word_ids:\n","        if word_id != current_word:\n","            # Start of a new word!\n","            current_word = word_id\n","            label = -100 if word_id is None else labels[word_id]\n","            new_labels.append(label)\n","        elif word_id is None:\n","            # Special token\n","            new_labels.append(-100)\n","        else:\n","            # Same word as previous token\n","            label = labels[word_id]\n","            # If the label is B-XXX we change it to I-XXX\n","            if label % 2 == 1:\n","                label += 1\n","            new_labels.append(label)\n","\n","    return new_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.607737Z","iopub.status.idle":"2023-10-20T17:35:45.608031Z","shell.execute_reply":"2023-10-20T17:35:45.607911Z","shell.execute_reply.started":"2023-10-20T17:35:45.607897Z"},"trusted":true},"outputs":[],"source":["def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(\n","        examples['tokens'], truncation=True, is_split_into_words=True\n","    )\n","    all_labels = examples['ner_tags']\n","    new_labels = []\n","    for i, labels in enumerate(all_labels):\n","        word_ids = tokenized_inputs.word_ids(i)\n","        new_labels.append(align_labels_with_tokens(labels, word_ids))\n","\n","    tokenized_inputs['labels'] = new_labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.609724Z","iopub.status.idle":"2023-10-20T17:35:45.610069Z","shell.execute_reply":"2023-10-20T17:35:45.609942Z","shell.execute_reply.started":"2023-10-20T17:35:45.609927Z"},"trusted":true},"outputs":[],"source":["tokenized_datasets = raw_ds.map(\n","    tokenize_and_align_labels,\n","    batched=True,\n","    remove_columns=raw_ds['train'].column_names,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.611050Z","iopub.status.idle":"2023-10-20T17:35:45.611585Z","shell.execute_reply":"2023-10-20T17:35:45.611264Z","shell.execute_reply.started":"2023-10-20T17:35:45.611244Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n","batch['labels']"]},{"cell_type":"markdown","metadata":{},"source":["## Training config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.625464Z","iopub.status.idle":"2023-10-20T17:35:45.625765Z","shell.execute_reply":"2023-10-20T17:35:45.625637Z","shell.execute_reply.started":"2023-10-20T17:35:45.625622Z"},"trusted":true},"outputs":[],"source":["output_dir = './models/ner/'\n","#repo = Repository(output_dir, clone_from=repo_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.614593Z","iopub.status.idle":"2023-10-20T17:35:45.615024Z","shell.execute_reply":"2023-10-20T17:35:45.614811Z","shell.execute_reply.started":"2023-10-20T17:35:45.614790Z"},"trusted":true},"outputs":[],"source":["metric = evaluate.load('seqeval')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.620336Z","iopub.status.idle":"2023-10-20T17:35:45.620764Z","shell.execute_reply":"2023-10-20T17:35:45.620564Z","shell.execute_reply.started":"2023-10-20T17:35:45.620543Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(eval_preds):\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    # Remove ignored index (special tokens) and convert to labels\n","    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n","    true_predictions = [\n","        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        'precision': all_metrics['overall_precision'],\n","        'recall': all_metrics['overall_recall'],\n","        'f1': all_metrics['overall_f1'],\n","        'accuracy': all_metrics['overall_accuracy'],\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_checkpoint, num_labels=67, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training_args = TrainingArguments(\n","#     output_dir=output_dir,\n","#     learning_rate=2e-5,\n","#     per_device_train_batch_size=8,\n","#     per_device_eval_batch_size=8,\n","#     num_train_epochs=10,\n","#     weight_decay=0.01,\n","#     evaluation_strategy=\"epoch\",\n","#     save_strategy=\"epoch\",\n","#     load_best_model_at_end=True,\n","# )\n","\n","# trainer = Trainer(\n","#     model=model,\n","#     args=training_args,\n","#     train_dataset=tokenized_datasets[\"train\"],\n","#     eval_dataset=tokenized_datasets[\"validation\"],\n","#     tokenizer=tokenizer,\n","#     data_collator=data_collator,\n","#     compute_metrics=compute_metrics,\n","# )\n","\n","# trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.616453Z","iopub.status.idle":"2023-10-20T17:35:45.616911Z","shell.execute_reply":"2023-10-20T17:35:45.616680Z","shell.execute_reply.started":"2023-10-20T17:35:45.616661Z"},"trusted":true},"outputs":[],"source":["#labels = raw_ds['train'][0]['ner_tags']\n","#labels = [label_names[i] for i in labels]\n","#labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.618218Z","iopub.status.idle":"2023-10-20T17:35:45.618651Z","shell.execute_reply":"2023-10-20T17:35:45.618453Z","shell.execute_reply.started":"2023-10-20T17:35:45.618433Z"},"trusted":true},"outputs":[],"source":["#predictions = labels.copy()\n","#predictions[2] = 'OTHER'\n","#metric.compute(predictions=[predictions], references=[labels])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.622061Z","iopub.status.idle":"2023-10-20T17:35:45.622476Z","shell.execute_reply":"2023-10-20T17:35:45.622286Z","shell.execute_reply.started":"2023-10-20T17:35:45.622265Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForTokenClassification.from_pretrained(\n","    model_checkpoint,\n","    id2label=id2label,\n","    label2id=label2id,\n","    ignore_mismatched_sizes=True\n",")\n","\n","train_dataloader = DataLoader(\n","    tokenized_datasets['train'],\n","    shuffle=True,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")\n","\n","eval_dataloader = DataLoader(\n","    tokenized_datasets['validation'], collate_fn=data_collator, batch_size=8\n",")\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","\n","# accelerator = Accelerator()\n","accelerator = Accelerator(cpu=True)\n","\n","model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n","    model, optimizer, train_dataloader, eval_dataloader\n",")\n","\n","num_train_epochs = 10\n","num_update_steps_per_epoch = len(train_dataloader)\n","num_training_steps = num_train_epochs * num_update_steps_per_epoch\n","\n","lr_scheduler = get_scheduler(\n","    'linear',\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.623537Z","iopub.status.idle":"2023-10-20T17:35:45.623794Z","shell.execute_reply":"2023-10-20T17:35:45.623681Z","shell.execute_reply.started":"2023-10-20T17:35:45.623668Z"},"trusted":true},"outputs":[],"source":["#model_name = 'bert-finetuned-ner-accelerate'\n","#repo_name = get_full_repo_name(model_name)\n","#repo_name"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.627183Z","iopub.status.idle":"2023-10-20T17:35:45.627452Z","shell.execute_reply":"2023-10-20T17:35:45.627337Z","shell.execute_reply.started":"2023-10-20T17:35:45.627324Z"},"trusted":true},"outputs":[],"source":["def postprocess(predictions, labels):\n","    predictions = predictions.detach().cpu().clone().numpy()\n","    labels = labels.detach().cpu().clone().numpy()\n","\n","    # Remove ignored index (special tokens) and convert to labels\n","    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n","    true_predictions = [\n","        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    return true_labels, true_predictions"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.628539Z","iopub.status.idle":"2023-10-20T17:35:45.628822Z","shell.execute_reply":"2023-10-20T17:35:45.628702Z","shell.execute_reply.started":"2023-10-20T17:35:45.628688Z"},"trusted":true},"outputs":[],"source":["progress_bar = tqdm(range(num_training_steps))\n","f1_score = []\n","\n","for epoch in range(num_train_epochs):\n","    # Training\n","    model.train()\n","    for batch in train_dataloader:\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        accelerator.backward(loss)\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)\n","\n","    # Evaluation\n","    model.eval()\n","    for batch in eval_dataloader:\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        predictions = outputs.logits.argmax(dim=-1)\n","        labels = batch['labels']\n","\n","        # Necessary to pad predictions and labels for being gathered\n","        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n","        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n","\n","        predictions_gathered = accelerator.gather(predictions)\n","        labels_gathered = accelerator.gather(labels)\n","\n","        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n","        metric.add_batch(predictions=true_predictions, references=true_labels)\n","\n","    results = metric.compute()\n","    f1_score.append(results['overall_f1'])\n","    print(\n","        f'epoch {epoch}:',\n","        {\n","            key: results[f'overall_{key}']\n","            for key in ['precision', 'recall', 'f1', 'accuracy']\n","        },\n","    )\n","\n","    #Save and upload\n","    accelerator.wait_for_everyone()\n","    unwrapped_model = accelerator.unwrap_model(model)\n","    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n","    if accelerator.is_main_process:\n","        tokenizer.save_pretrained(output_dir)\n","        #repo.push_to_hub(\n","        #    commit_message=f'Training in progress epoch {epoch}', blocking=False\n","        #)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(results)\n","# {'AGE': {'precision': 0.9565217391304348, 'recall': 0.88, 'f1': 0.9166666666666666, 'number': 25}, 'CITY': {'precision': 0.9893048128342246, 'recall': 0.9840425531914894, 'f1': 0.9866666666666667, 'number': 188}, 'DATE': {'precision': 0.9914984059511158, 'recall': 0.9978609625668449, 'f1': 0.9946695095948828, 'number': 935}, 'DEPARTMENT': {'precision': 0.9478672985781991, 'recall': 0.9302325581395349, 'f1': 0.9389671361502349, 'number': 215}, 'DOCTOR': {'precision': 0.9845201238390093, 'recall': 0.9845201238390093, 'f1': 0.9845201238390093, 'number': 1292}, 'DURATION': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3}, 'HOSPITAL': {'precision': 0.9827089337175793, 'recall': 0.9798850574712644, 'f1': 0.981294964028777, 'number': 348}, 'IDNUM': {'precision': 0.9876373626373627, 'recall': 0.9930939226519337, 'f1': 0.9903581267217632, 'number': 724}, 'LOCATION-OTHER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'MEDICALRECORD': {'precision': 0.9971830985915493, 'recall': 0.9943820224719101, 'f1': 0.9957805907172996, 'number': 356}, 'ORGANIZATION': {'precision': 0.9090909090909091, 'recall': 0.8333333333333334, 'f1': 0.8695652173913043, 'number': 24}, 'PATIENT': {'precision': 0.9831460674157303, 'recall': 0.9915014164305949, 'f1': 0.9873060648801127, 'number': 353}, 'PHONE': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'SET': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'STATE': {'precision': 0.9942528735632183, 'recall': 0.9885714285714285, 'f1': 0.991404011461318, 'number': 175}, 'STREET': {'precision': 0.9888888888888889, 'recall': 0.978021978021978, 'f1': 0.9834254143646408, 'number': 182}, 'TIME': {'precision': 0.9772727272727273, 'recall': 0.9641255605381166, 'f1': 0.9706546275395034, 'number': 223}, 'ZIP': {'precision': 1.0, 'recall': 0.9731182795698925, 'f1': 0.9863760217983651, 'number': 186}, 'overall_precision': 0.9833652007648184, 'overall_recall': 0.9835532606616944, 'overall_f1': 0.9834592217229181, 'overall_accuracy': 0.999238331528254}"]},{"cell_type":"markdown","metadata":{},"source":["## Draw f1 score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f1_dict = {'AGE': {'precision': 0.9565217391304348, 'recall': 0.88, 'f1': 0.9166666666666666, 'number': 25}, 'CITY': {'precision': 0.9893048128342246, 'recall': 0.9840425531914894, 'f1': 0.9866666666666667, 'number': 188}, 'DATE': {'precision': 0.9914984059511158, 'recall': 0.9978609625668449, 'f1': 0.9946695095948828, 'number': 935}, 'DEPARTMENT': {'precision': 0.9478672985781991, 'recall': 0.9302325581395349, 'f1': 0.9389671361502349, 'number': 215}, 'DOCTOR': {'precision': 0.9845201238390093, 'recall': 0.9845201238390093, 'f1': 0.9845201238390093, 'number': 1292}, 'DURATION': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3}, 'HOSPITAL': {'precision': 0.9827089337175793, 'recall': 0.9798850574712644, 'f1': 0.981294964028777, 'number': 348}, 'IDNUM': {'precision': 0.9876373626373627, 'recall': 0.9930939226519337, 'f1': 0.9903581267217632, 'number': 724}, 'LOCATION-OTHER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'MEDICALRECORD': {'precision': 0.9971830985915493, 'recall': 0.9943820224719101, 'f1': 0.9957805907172996, 'number': 356}, 'ORGANIZATION': {'precision': 0.9090909090909091, 'recall': 0.8333333333333334, 'f1': 0.8695652173913043, 'number': 24}, 'PATIENT': {'precision': 0.9831460674157303, 'recall': 0.9915014164305949, 'f1': 0.9873060648801127, 'number': 353}, 'PHONE': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'SET': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'STATE': {'precision': 0.9942528735632183, 'recall': 0.9885714285714285, 'f1': 0.991404011461318, 'number': 175}, 'STREET': {'precision': 0.9888888888888889, 'recall': 0.978021978021978, 'f1': 0.9834254143646408, 'number': 182}, 'TIME': {'precision': 0.9772727272727273, 'recall': 0.9641255605381166, 'f1': 0.9706546275395034, 'number': 223}, 'ZIP': {'precision': 1.0, 'recall': 0.9731182795698925, 'f1': 0.9863760217983651, 'number': 186}, 'overall_precision': 0.9833652007648184, 'overall_recall': 0.9835532606616944, 'overall_f1': 0.9834592217229181, 'overall_accuracy': 0.999238331528254}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pert_f1_df = pd.DataFrame({\n","    'Tag': [],\n","    'Precision': [],\n","    'Recall': [],\n","    'F1': [],\n","    'Number': []\n","})\n","\n","for tag, data in f1_dict.items():\n","    new_row = []\n","    overall = []\n","\n","    if tag.find('overall_') != -1:\n","        continue\n","\n","    new_row.append(tag)\n","\n","    for key, val in data.items():\n","        new_row.append(val)\n","    \n","    pert_f1_df.loc[len(pert_f1_df.index)] = new_row\n","\n","pert_f1_df.loc[len(pert_f1_df.index)] = ['', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n","pert_f1_df.loc[len(pert_f1_df.index)] = ['', 0.9833652007648184, 0.9835532606616944, 0.9834592217229181, 0.999238331528254]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pert_f1_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pert_f1_df.to_csv('./Validation_Dataset/pert_ans/f1.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-20T17:35:45.630322Z","iopub.status.idle":"2023-10-20T17:35:45.630593Z","shell.execute_reply":"2023-10-20T17:35:45.630474Z","shell.execute_reply.started":"2023-10-20T17:35:45.630461Z"},"trusted":true},"outputs":[],"source":["model_name = model_name.replace('/', '_')\n","plt.plot(f1_score, label = \"f1\")\n","# naming the x axis\n","plt.xlabel('epoch')\n","# naming the y axis\n","plt.ylabel('f1 score')\n","# giving a title to my graph\n","title = f'{model_name}'\n","plt.title(title)\n","# show a legend on the plot\n","plt.legend()\n","# store fig\n","# plt.savefig(model_name)\n","# function to show the plot\n","plt.show()\n","# store score\n","# with open(title, \"wb\") as fp:   #Pickling\n","#     pickle.dump(f1_score, fp)"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_docs = {'id':[], 'doc':[]}\n","fnames = [f for f in os.listdir('./Validation_Dataset/Validation_Release/')]\n","fnames.sort()\n","\n","# max_word_length = 80\n","# fa = open('./Second_Phase_Dataset/answer.txt', 'r')\n","for fname in tqdm(fnames):\n","    f = open(f'./Validation_Dataset/Validation_Release/{fname}', 'r')\n","    lines = f.read()\n","    # tok = lines.split()\n","\n","    val_docs['id'].append(fname[:-4])\n","    val_docs['doc'].append(lines)\n","\n","    # tok, ner = Spilt2Words(fname[:-4], f, fa)\n","    # if (max_word_length > 0):\n","    #     Segmentation(ds_dict['id'], ds_dict['tokens'], ds_dict['ner_tags'], fname[:-4], tok, ner, max_word_length)\n","    # else:\n","    #     ds_dict['id'].append(fname[:-4])\n","    #     ds_dict['tokens'].append(tok)\n","    #     ds_dict['ner_tags'].append(ner)\n","    f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","# Download the sentence tokenizer model (run this once)\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","def split_documents(fnames, words_per_segment):\n","    result_dict = {}\n","\n","    for fname in tqdm(fnames):\n","        with open(os.path.join('./Validation_Dataset/Validation_Release', fname), 'r') as file:\n","            content = file.read()\n","\n","        current_segment = []\n","        segments = []\n","        word_count = 0\n","\n","        # Use a regular expression to split the content into words\n","        words = content.split(\" \")\n","\n","        for word in words:\n","            # Check if splitting is needed based on word count\n","            word_count += 1\n","            if word_count > words_per_segment:\n","                key = f\"{fname[:-4]}_{len(segments) + 1}\"\n","                result_dict[key] = ' '.join(current_segment)\n","                current_segment = []\n","                segments.append(key)\n","                word_count = 0\n","\n","            current_segment.append(word)\n","\n","        # Handle the remaining words after the loop\n","        if current_segment:\n","            key = f\"{fname[:-4]}_{len(segments) + 1}\"\n","            result_dict[key] = ' '.join(current_segment)\n","            segments.append(key)\n","\n","    return result_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fnames = [f for f in os.listdir('./Validation_Dataset/Validation_Release/')]\n","fnames.sort()\n","\n","max_lines_per_segment = 10\n","max_sentences_per_segment = 5\n","max_characters_per_segment = 100\n","words_per_segment = 80\n","\n","# result_segments = split_documents(fnames, max_lines_per_segment, max_sentences_per_segment)\n","\n","val_result_segments = split_documents(fnames, words_per_segment)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print the first segment of the first document for demonstration\n","key_example = list(val_result_segments.keys())[2]\n","print(f\"Segment {key_example}:\")\n","print(val_result_segments[key_example])\n","\n","# val_docs['doc'][0][1855:].count('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# list[val_result_segments.keys()]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print(len(sent_tokenize(result_segments['650_8'])))\n","# print(val_result_segments['file21703_12'])"]},{"cell_type":"markdown","metadata":{},"source":["### Load model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import pipeline\n","\n","# Replace this with your own checkpoint\n","model_checkpoint = \"./models/ner/\"\n","token_classifier = pipeline(\n","    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# classify\n","val_result_ans_dict = {}\n","\n","for fid_sid, seg in val_result_segments.items():\n","    try:\n","        val_result_ans_dict[fid_sid] = token_classifier(seg)\n","    except:\n","        print(fid_sid)\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# val_result_ans_dict['1002_6']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# result_ans_dict_cpy = result_ans_dict.copy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","from word2number import w2n\n","\n","def Normalize(time_type, org):\n","    nor = ''\n","    if (time_type == 'DATE'):\n","        if (re.match('\\d{1,2}(\\/|\\.| |-|,)\\d{1,2}(\\/|\\.| |-|,)\\d{2,4}', org)):\n","            l = re.split('\\/|\\.| |-|,', org)\n","            if (len(l[2]) == 2):\n","                l[2] = '20' + l[2]\n","            elif (len(l[2]) == 3):\n","                l[2] = '2' + l[2]\n","            if (len(l[1]) == 1):\n","                l[1] = '0' + l[1]\n","            if (len(l[0]) == 1):\n","                l[0] = '0' + l[0]\n","            nor = l[2] + '-' + l[1] + '-' + l[0]\n","        elif (re.match('\\/\\d{1,2}\\/(\\d{2}|\\d{4})', org)):\n","            l = re.split('\\/', org)\n","            if (len(l[1]) == 1):\n","                l[1] = '0' + l[1]\n","            if (len(l[2]) == 2):\n","                l[2] = '20' + l[2]\n","            nor = l[2] + '-' + l[1]\n","        elif (re.match('\\d{1,2}\\/\\d{2,5}', org)):\n","            l = re.split('\\/', org)\n","            if (len(l[0]) == 1):\n","                l[0] = '0' + l[0]\n","            if (len(l[1]) == 2):\n","                nor = '20' + l[1] + '-' + l[0]\n","            elif (len(l[1]) == 3):\n","                nor = '20' + l[1][1:] + '-' + '0' + l[1][0] + '-' + l[0]\n","            elif (len(l[1]) == 4):\n","                nor = l[1] + '-' + l[0]\n","            elif (len(l[1]) == 5):\n","                nor = l[1][1:] + '-' + '0' + l[1][0] + '-' + l[0]\n","        elif (re.match('\\d{8}', org)):\n","            nor = org[0:4] + '-' + org[4:6] + '-' + org[6:8]\n","        elif (re.match('\\d{4}', org)):\n","            nor = org\n","        elif (re.match('\\d{3}', org)):\n","            nor = '2' + org\n","        elif (re.match('(\\d{2}|)(-|)(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-| )\\d{2,4}', org)):\n","            org = org.replace('Jan', '01')\n","            org = org.replace('Feb', '02')\n","            org = org.replace('Mar', '03')\n","            org = org.replace('Apr', '04')\n","            org = org.replace('May', '05')\n","            org = org.replace('Jun', '06')\n","            org = org.replace('Jul', '07')\n","            org = org.replace('Aug', '08')\n","            org = org.replace('Sep', '09')\n","            org = org.replace('Oct', '10')\n","            org = org.replace('Nov', '11')\n","            org = org.replace('Dec', '12')\n","            l = re.split('-| ', org)\n","            if (len(l) == 2):\n","                if (len(l[1]) == 2):\n","                    l[1] = '20' + l[1]\n","                elif (len(l[1]) == 3):\n","                    l[1] = '2' + l[1]\n","                nor = l[1] + '-' + l[0]\n","            else:\n","                if (len(l[2]) == 2):\n","                    l[2] = '20' + l[2]\n","                elif (len(l[2]) == 3):\n","                    l[2] = '2' + l[2]\n","                nor = l[2] + '-' + l[1] + '-' + l[0]\n","        elif (re.match('\\d{1,2}((st)|(nd)|(rd)|(th)) of (January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}', org)):\n","            org = org.replace('January', '01')\n","            org = org.replace('Feburary', '02')\n","            org = org.replace('March', '03')\n","            org = org.replace('April', '04')\n","            org = org.replace('May', '05')\n","            org = org.replace('June', '06')\n","            org = org.replace('July', '07')\n","            org = org.replace('August', '08')\n","            org = org.replace('September', '09')\n","            org = org.replace('October', '10')\n","            org = org.replace('November', '11')\n","            org = org.replace('December', '12')\n","            l = re.split(' ', org)\n","            nor = l[3] + '-' + l[2] + '-' + l[0][:-2]\n","        elif (re.match('(\\d{1,2}|)( |)(January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}', org)):\n","            if (re.match('\\d', org[0]) and re.match('\\d', org[1]) == None):\n","                org = '0' + org\n","            org = org.replace('January', '01')\n","            org = org.replace('Feburary', '02')\n","            org = org.replace('March', '03')\n","            org = org.replace('April', '04')\n","            org = org.replace('May', '05')\n","            org = org.replace('June', '06')\n","            org = org.replace('July', '07')\n","            org = org.replace('August', '08')\n","            org = org.replace('September', '09')\n","            org = org.replace('October', '10')\n","            org = org.replace('November', '11')\n","            org = org.replace('December', '12')\n","            org = org.replace(' ', '')\n","            if (len(org) == 6):\n","                nor = org[2:] + '-' + org[0:2]\n","            else:    \n","                nor = org[4:] + '-' + org[2:4] + '-' + org[0:2]\n","    elif (time_type == 'TIME'):\n","        if (re.match('(\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}(  | |)|)(at|)( |)\\d{1,2}(:|\\.)\\d{2}(AM|am|PM|pm|Hr|Hrs|hr|hrs|)( on the \\d{1,2}((st)|(nd)|(rd)|(th)) of (January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}|)', org)):\n","            tmp = org\n","            pm = 0\n","            am = 0\n","            if (re.search('PM', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('AM', org, flags=0) != None):\n","                am = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            get_date = 0\n","            date = re.search('\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org, flags=0)\n","            if (date != None):\n","                date = date.group(0)\n","                org = org.replace(date, '')\n","                date = re.split('\\/|\\.', date)\n","                if (len(date[0]) == 1):\n","                    date[0] = '0' + date[0]\n","                if (len(date[1]) == 1):\n","                    date[1] = '0' + date[1]\n","                if (len(date[2]) == 2):\n","                    date[2] = '20' + date[2]\n","                elif (len(date[2]) == 3):\n","                    date[2] = '2' + date[2]\n","                nor = date[2] + '-' + date[1] + '-' + date[0]\n","                get_date = 1\n","            yyyy = re.search('\\d{4}', org, flags=0)\n","            if (yyyy != None and get_date == 0):\n","                yyyy = yyyy.group(0)\n","                org = org.replace(yyyy, '')\n","                nor = yyyy + '-'\n","            mm = re.search('January|February|March|April|May|June|July|August|September|October|November|December', org, flags=0)\n","            if (mm != None and get_date == 0):\n","                mm = mm.group(0)\n","                org = org.replace(mm, '')\n","                mm = mm.replace('January', '01')\n","                mm = mm.replace('Feburary', '02')\n","                mm = mm.replace('March', '03')\n","                mm = mm.replace('April', '04')\n","                mm = mm.replace('May', '05')\n","                mm = mm.replace('June', '06')\n","                mm = mm.replace('July', '07')\n","                mm = mm.replace('August', '08')\n","                mm = mm.replace('September', '09')\n","                mm = mm.replace('October', '10')\n","                mm = mm.replace('November', '11')\n","                mm = mm.replace('December', '12')\n","                nor = nor + mm + '-'\n","            dd = re.search('\\d{1,2}((st)|(nd)|(rd)|(th))', org, flags=0)\n","            if (dd != None and get_date == 0):\n","                dd = dd.group(0)\n","                org = org.replace(dd, '')\n","                dd = dd.replace('st', '')\n","                dd = dd.replace('nd', '')\n","                dd = dd.replace('rd', '')\n","                dd = dd.replace('th', '')\n","                if (len(dd) == 1):\n","                    dd = '0' + dd\n","                nor = nor + dd\n","            get_time = 0\n","            time = re.search('\\d{1,2}(:|\\.)\\d{1,2}', org, flags=0)\n","            if (time != None):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                time = re.split('\\.|:', time)\n","                if (pm == 1 and int(time[0]) < 12):\n","                    time[0] = str(int(time[0]) + 12)\n","                elif (am == 1 and int(time[0]) == 12):\n","                    time[0] = '00'\n","                if (len(time[0]) == 1):\n","                    time[0] = '0' + time[0]\n","                nor = nor + 'T' + time[0] + ':' + time[1]\n","                get_time = 1\n","            pm = 0\n","            am = 0\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            time = re.search('\\d{1,4}', org, flags=0)\n","            if (time != None and get_time == 0):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                hh, mm = '00', '00'\n","                if (len(time) == 4):\n","                    hh = time[0:2]\n","                    mm = time[2:]\n","                elif (len(time) == 3):\n","                    hh = time[0]\n","                    mm = time[1:]\n","                elif (len(time) == 2):\n","                    hh = time\n","                elif (len(time) == 1):\n","                    hh = time\n","                if (pm == 1 and int(hh) < 12):\n","                    hh = str(int(hh) + 12)\n","                elif (am == 1 and int(hh) == 12):\n","                    hh = '00'\n","                nor = nor + 'T' + hh + ':' + mm    \n","            #if (nor != ans):    \n","                #print(f'1:nor={nor}, ans={ans}, org={tmp}')\n","        elif (re.match('\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', org)):\n","            tmp = org\n","            nor = org.replace(' ', 'T')\n","            #if (nor != ans):    \n","                #print(f'2:nor={nor}, ans={ans}, org={tmp}')\n","        elif (re.match('(at |)(\\d{1,2}|)(:|\\.|)\\d{2}( |)(am|pm|Hr|Hrs|hr|hrs|)( on | )(the |)\\d{1,2}(\\/|\\.)\\d{2,4}(\\/|\\.)\\d{1,2}', org)):\n","            tmp = org\n","            pm = 0\n","            am = 0\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            date = re.search('\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org, flags=0)\n","            if (date != None):\n","                date = date.group(0)\n","                org = org.replace(date, '')\n","                date = re.split('\\/|\\.', date)\n","                if (len(date[0]) == 1):\n","                    date[0] = '0' + date[0]\n","                if (len(date[1]) == 1):\n","                    date[1] = '0' + date[1]\n","                if (len(date[2]) == 2):\n","                    date[2] = '20' + date[2]\n","                elif (len(date[2]) == 3):\n","                    date[2] = '2' + date[2]\n","                nor = date[2] + '-' + date[1] + '-' + date[0] + 'T'\n","            org = org.replace(':', '')\n","            time = re.search('\\d{1,4}', org, flags=0)\n","            if (time != None):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                hh, mm = '00', '00'\n","                if (len(time) == 4):\n","                    hh = time[0:2]\n","                    mm = time[2:]\n","                elif (len(time) == 3):\n","                    hh = time[0]\n","                    mm = time[1:]\n","                elif (len(time) == 2):\n","                    hh = time\n","                elif (len(time) == 1):\n","                    hh = time\n","                if (pm == 1 and int(hh) < 12):\n","                    hh = str(int(hh) + 12)\n","                elif (am == 1 and int(hh) == 12):\n","                    hh = '00'\n","                nor = nor + hh + ':' + mm\n","            #if (nor != ans):    \n","                #print(f'3:nor={nor}, ans={ans}, org={tmp}')\n","        elif (re.match('((\\d{1,2}((pm)|(am)))|(\\d{4}(Hr|Hrs|hr|hrs|)))(( on )| )\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org)):\n","            tmp = org\n","            pm = 0\n","            am = 0\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            date = re.search('\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org, flags=0)\n","            if (date != None):\n","                date = date.group(0)\n","                org = org.replace(date, '')\n","                date = re.split('\\/|\\.', date)\n","                if (len(date[0]) == 1):\n","                    date[0] = '0' + date[0]\n","                if (len(date[1]) == 1):\n","                    date[1] = '0' + date[1]\n","                if (len(date[2]) == 2):\n","                    date[2] = '20' + date[2]\n","                elif (len(date[2]) == 3):\n","                    date[2] = '2' + date[2]\n","                nor = date[2] + '-' + date[1] + '-' + date[0] + 'T'\n","            hrtime = re.search('\\d{4}', org, flags=0)\n","            if (hrtime != None):\n","                hrtime = hrtime.group(0)\n","                org = org.replace(hrtime, '')\n","                nor = nor + hrtime[0:2] + ':' + hrtime[2:]\n","            time = re.search('\\d{1,2}', org, flags=0)\n","            if (time != None):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                hh = time\n","                if (pm == 1 and int(hh) < 12):\n","                    hh = str(int(hh) + 12)\n","                elif (am == 1 and int(hh) == 12):\n","                    hh = '00'\n","                if (len(hh) == 1):\n","                    hh = '0' + hh\n","                nor = nor + hh + ':' + '00'\n","            #if (nor != ans):    \n","                #print(f'4:nor={nor}, ans={ans}, org={tmp}')\n","    elif (time_type == 'DURATION'):   \n","        tmp = org\n","        org = org.replace('one', '1')\n","        org = org.replace('two', '2')\n","        org = org.replace('three', '3')\n","        org = org.replace('four', '4')\n","        org = org.replace('five', '5')\n","        num = ''\n","        alp = ''\n","        space_idx = org.find(' ')\n","        for i in range(len(org)):\n","            if (org[i] == 'D' or org[i] == 'd' or\\\n","                org[i] == 'W' or org[i] == 'w' or\\\n","                org[i] == 'M' or org[i] == 'm' or\\\n","                org[i] == 'Y' or org[i] == 'y') and i > space_idx:\n","                alp = org[i]\n","                org = org[:i]\n","                break\n","        # print(org, alp)\n","        org = re.split('-| ', org)\n","        try:\n","            if org[0].isalpha():\n","                org[0] = w2n.word_to_num(org[0])\n","            # print(org)\n","            if (len(org) == 1 or org[1] == ''):\n","                nor = 'P' + str(org[0]) + alp.upper()\n","            else:\n","                nor = 'P' + str((int(org[0]) + int(org[1])) / 2) + alp.upper()\n","        except:\n","            nor = tmp\n","        # if (nor != ans):    \n","        #     print(f'dur:nor={nor}, ans={ans}, org={tmp}')\n","    elif (time_type == 'SET'):\n","        if (re.match('twice', org)):\n","            nor = 'R2'\n","    return nor\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_ans_df = pd.DataFrame({\n","    'file_id': [],\n","    'PHI_type': [],\n","    'PHI_start': [],\n","    'PHI_end': [],\n","    'PHI_content': [],\n","    'ISO': []\n","})\n","\n","last_fid = \"\"\n","last_idx_of_last_seg = 0\n","for fid_sid, entities in val_result_ans_dict.items():\n","    curr_fid = fid_sid.split('_')[0]\n","    curr_sid = fid_sid.split('_')[1]\n","    # print(fid_sid)\n","\n","    if curr_fid != last_fid:\n","        with open(os.path.join('./Validation_Dataset/Validation_Release', curr_fid+'.txt'), 'r') as file:\n","            content = file.read()\n","        last_fid = curr_fid\n","        last_idx_of_last_seg = 0\n","\n","    # last_idx_of_last_seg = 0\n","\n","    for i, entity in enumerate(entities):\n","        new_row = []\n","        # print(i, entity)\n","\n","        if i == len(entities) - 1 and entity['entity_group'] == 'OTHER':\n","            last_idx_of_last_seg += len(val_result_segments[fid_sid])\n","            continue\n","        elif entity['entity_group'] != 'OTHER':\n","            start_idx = entity['start'] + last_idx_of_last_seg + int(curr_sid) - 1\n","            end_idx = entity['end'] + last_idx_of_last_seg + int(curr_sid) - 1\n","            \n","            word = content[start_idx:end_idx]\n","\n","            if i == len(entities) - 1:\n","                last_idx_of_last_seg += len(val_result_segments[fid_sid])\n","\n","            if len(word) > 1:\n","                # print(word, start_idx, end_idx)\n","                while word[0].isalnum() == False or word[-1].isalnum() == False:\n","                    if word[0].isalnum() == False:\n","                        word = word[1:]\n","                        start_idx += 1\n","                    # print(word, start_idx, end_idx)\n","                    if word[-1].isalnum() == False:\n","                        word = word[:-1]\n","                        end_idx -= 1\n","            \n","            if '\\n' in word:\n","                word = word.replace('\\n', ' ')\n","\n","            label = entity['entity_group']\n","\n","            have_num_or_aplha_desc = False\n","            if word[0].isdigit() and word[-1].isalpha and content[start_idx-1] == ' ': \n","                have_num_or_aplha_desc = True\n","            elif word.find(' ') != -1:\n","                have_num_or_aplha_desc = True\n","\n","            if entity['entity_group'] == 'DURATION' and word != 'twice' and have_num_or_aplha_desc == False:\n","                word_cpy = word.lower()\n","                if word_cpy.find('yr') != -1 or word_cpy.find('ye') != -1 or word_cpy.find('m') != -1 or word_cpy.find('w') != -1:\n","                    last_index = start_idx - 1\n","                    # case 1: no spcae between number and month, week or year\n","                    if content[last_index].isdigit():\n","                        while content[last_index].isdigit():\n","                            last_index -= 1\n","                        start_idx = last_index + 1\n","                        word = content[start_idx:end_idx]\n","                    else:\n","                        last_space1 = content.rfind(' ', 0, start_idx)\n","                        last_space2 = content.rfind(' ', 0, last_space1)\n","                        start_idx = last_space2 + 1\n","                        word = content[start_idx:end_idx]\n","                elif word.isdigit():\n","                    next_index = end_idx + 1\n","                    if content[next_index].isalpha():\n","                        while content[next_index].isalpha():\n","                            next_index += 1\n","                        end_idx = next_index\n","                        word = content[start_idx:end_idx]\n","                    else:\n","                        first_space_index = content.find(' ', end_idx)\n","                        # Find the index of the second space after the target word\n","                        second_space_index = content.find(' ', first_space_index + 1)\n","                        end_idx = second_space_index\n","                        word = content[start_idx:end_idx]   \n","            elif word == 'twice':\n","                label = 'SET'\n","            # if word == 'twice':\n","            #     label = 'SET'\n","            \n","            \n","            new_row.extend([curr_fid, label, start_idx, end_idx, word])\n","            \n","\n","            need_iso = ['DATE', 'TIME', 'DURATION', 'SET']\n","\n","            if entity['entity_group'] in need_iso:\n","                new_row.append(Normalize(label, word))\n","            else:\n","                new_row.append('')\n","            \n","            val_ans_df.loc[len(val_ans_df)] = new_row\n","        else:\n","            continue\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_ans_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_ans_df.loc[val_ans_df['PHI_type'] == 'DURATION']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# with open('')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_crf_val = pd.read_csv('./Validation_Dataset/prediction.csv')\n","df_crf_duration = df_crf_val.loc[df_crf_val['PHI_type'] == 'DURATION']\n","df_crf_lo = df_crf_val.loc[df_crf_val['PHI_type'] == 'LOCATION-OTHER']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_crf_duration"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_ans_df.to_csv('./Validation_Dataset/pert_ans/pert_answer.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_ans_df.to_csv('./Validation_Dataset/pert_ans/pert_answer.txt', sep='\\t', header=False, index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Inference: Test set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_docs = {'id':[], 'doc':[]}\n","fnames = [f for f in os.listdir('./opendid_test/opendid_test/')]\n","fnames.sort()\n","\n","# max_word_length = 80\n","# fa = open('./Second_Phase_Dataset/answer.txt', 'r')\n","for fname in tqdm(fnames):\n","    f = open(f'./opendid_test/opendid_test/{fname}', 'r')\n","    lines = f.read()\n","    # tok = lines.split()\n","\n","    test_docs['id'].append(fname[:-4])\n","    test_docs['doc'].append(lines)\n","\n","    f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","# Download the sentence tokenizer model (run this once)\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","def split_documents(fnames, words_per_segment):\n","    result_dict = {}\n","\n","    for fname in tqdm(fnames):\n","        with open(os.path.join('./opendid_test/opendid_test', fname), 'r') as file:\n","            content = file.read()\n","\n","        current_segment = []\n","        segments = []\n","        word_count = 0\n","\n","        # Use a regular expression to split the content into words\n","        words = content.split(\" \")\n","\n","        for word in words:\n","            # Check if splitting is needed based on word count\n","            word_count += 1\n","            if word_count > words_per_segment:\n","                key = f\"{fname[:-4]}_{len(segments) + 1}\"\n","                result_dict[key] = ' '.join(current_segment)\n","                current_segment = []\n","                segments.append(key)\n","                word_count = 0\n","\n","            current_segment.append(word)\n","\n","        # Handle the remaining words after the loop\n","        if current_segment:\n","            key = f\"{fname[:-4]}_{len(segments) + 1}\"\n","            result_dict[key] = ' '.join(current_segment)\n","            segments.append(key)\n","\n","    return result_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fnames = [f for f in os.listdir('./opendid_test/opendid_test/')]\n","fnames.sort()\n","\n","max_lines_per_segment = 10\n","max_sentences_per_segment = 5\n","max_characters_per_segment = 100\n","words_per_segment = 80\n","\n","# result_segments = split_documents(fnames, max_lines_per_segment, max_sentences_per_segment)\n","\n","result_segments = split_documents(fnames, words_per_segment)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print the first segment of the first document for demonstration\n","key_example = list(result_segments.keys())[0]\n","print(f\"Segment {key_example}:\")\n","print(result_segments[key_example])\n","\n","# val_docs['doc'][0][1855:].count('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# list[result_segments.keys()]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print(len(sent_tokenize(result_segments['650_8'])))\n","# print(result_segments['file21703_12'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import pipeline\n","\n","# Replace this with your own checkpoint\n","model_checkpoint = \"./models/ner/\"\n","token_classifier = pipeline(\n","    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result_ans_dict = {}\n","\n","for fid_sid, seg in result_segments.items():\n","    try:\n","        result_ans_dict[fid_sid] = token_classifier(seg)\n","    except:\n","        print(fid_sid)\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# result_ans_dict['1002_6']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","from word2number import w2n\n","\n","def Normalize(time_type, org):\n","    nor = ''\n","    if (time_type == 'DATE'):\n","        if (re.match('\\d{1,2}(\\/|\\.| |-|,)\\d{1,2}(\\/|\\.| |-|,)\\d{2,4}', org)):\n","            l = re.split('\\/|\\.| |-|,', org)\n","            if (len(l[2]) == 2):\n","                l[2] = '20' + l[2]\n","            elif (len(l[2]) == 3):\n","                l[2] = '2' + l[2]\n","            if (len(l[1]) == 1):\n","                l[1] = '0' + l[1]\n","            if (len(l[0]) == 1):\n","                l[0] = '0' + l[0]\n","            nor = l[2] + '-' + l[1] + '-' + l[0]\n","        elif (re.match('\\/\\d{1,2}\\/(\\d{2}|\\d{4})', org)):\n","            l = re.split('\\/', org)\n","            if (len(l[1]) == 1):\n","                l[1] = '0' + l[1]\n","            if (len(l[2]) == 2):\n","                l[2] = '20' + l[2]\n","            nor = l[2] + '-' + l[1]\n","        elif (re.match('\\d{1,2}\\/\\d{2,5}', org)):\n","            l = re.split('\\/', org)\n","            if (len(l[0]) == 1):\n","                l[0] = '0' + l[0]\n","            if (len(l[1]) == 2):\n","                nor = '20' + l[1] + '-' + l[0]\n","            elif (len(l[1]) == 3):\n","                nor = '20' + l[1][1:] + '-' + '0' + l[1][0] + '-' + l[0]\n","            elif (len(l[1]) == 4):\n","                nor = l[1] + '-' + l[0]\n","            elif (len(l[1]) == 5):\n","                nor = l[1][1:] + '-' + '0' + l[1][0] + '-' + l[0]\n","        elif (re.match('\\d{8}', org)):\n","            nor = org[0:4] + '-' + org[4:6] + '-' + org[6:8]\n","        elif (re.match('\\d{4}', org)):\n","            nor = org\n","        elif (re.match('\\d{3}', org)):\n","            nor = '2' + org\n","        elif (re.match('(\\d{2}|)(-|)(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-| )\\d{2,4}', org)):\n","            org = org.replace('Jan', '01')\n","            org = org.replace('Feb', '02')\n","            org = org.replace('Mar', '03')\n","            org = org.replace('Apr', '04')\n","            org = org.replace('May', '05')\n","            org = org.replace('Jun', '06')\n","            org = org.replace('Jul', '07')\n","            org = org.replace('Aug', '08')\n","            org = org.replace('Sep', '09')\n","            org = org.replace('Oct', '10')\n","            org = org.replace('Nov', '11')\n","            org = org.replace('Dec', '12')\n","            l = re.split('-| ', org)\n","            if (len(l) == 2):\n","                if (len(l[1]) == 2):\n","                    l[1] = '20' + l[1]\n","                elif (len(l[1]) == 3):\n","                    l[1] = '2' + l[1]\n","                nor = l[1] + '-' + l[0]\n","            else:\n","                if (len(l[2]) == 2):\n","                    l[2] = '20' + l[2]\n","                elif (len(l[2]) == 3):\n","                    l[2] = '2' + l[2]\n","                nor = l[2] + '-' + l[1] + '-' + l[0]\n","        elif (re.match('\\d{1,2}((st)|(nd)|(rd)|(th)) of (January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}', org)):\n","            org = org.replace('January', '01')\n","            org = org.replace('Feburary', '02')\n","            org = org.replace('March', '03')\n","            org = org.replace('April', '04')\n","            org = org.replace('May', '05')\n","            org = org.replace('June', '06')\n","            org = org.replace('July', '07')\n","            org = org.replace('August', '08')\n","            org = org.replace('September', '09')\n","            org = org.replace('October', '10')\n","            org = org.replace('November', '11')\n","            org = org.replace('December', '12')\n","            l = re.split(' ', org)\n","            nor = l[3] + '-' + l[2] + '-' + l[0][:-2]\n","        elif (re.match('(\\d{1,2}|)( |)(January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}', org)):\n","            if (re.match('\\d', org[0]) and re.match('\\d', org[1]) == None):\n","                org = '0' + org\n","            org = org.replace('January', '01')\n","            org = org.replace('Feburary', '02')\n","            org = org.replace('March', '03')\n","            org = org.replace('April', '04')\n","            org = org.replace('May', '05')\n","            org = org.replace('June', '06')\n","            org = org.replace('July', '07')\n","            org = org.replace('August', '08')\n","            org = org.replace('September', '09')\n","            org = org.replace('October', '10')\n","            org = org.replace('November', '11')\n","            org = org.replace('December', '12')\n","            org = org.replace(' ', '')\n","            if (len(org) == 6):\n","                nor = org[2:] + '-' + org[0:2]\n","            else:    \n","                nor = org[4:] + '-' + org[2:4] + '-' + org[0:2]\n","    elif (time_type == 'TIME'):\n","        if (re.match('(\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}(  | |)|)(at|)( |)\\d{1,2}(:|\\.)\\d{2}(AM|am|PM|pm|Hr|Hrs|hr|hrs|)( on the \\d{1,2}((st)|(nd)|(rd)|(th)) of (January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}|)', org)):\n","            tmp = org\n","            pm = 0\n","            am = 0\n","            if (re.search('PM', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('AM', org, flags=0) != None):\n","                am = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            get_date = 0\n","            date = re.search('\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org, flags=0)\n","            if (date != None):\n","                date = date.group(0)\n","                org = org.replace(date, '')\n","                date = re.split('\\/|\\.', date)\n","                if (len(date[0]) == 1):\n","                    date[0] = '0' + date[0]\n","                if (len(date[1]) == 1):\n","                    date[1] = '0' + date[1]\n","                if (len(date[2]) == 2):\n","                    date[2] = '20' + date[2]\n","                elif (len(date[2]) == 3):\n","                    date[2] = '2' + date[2]\n","                nor = date[2] + '-' + date[1] + '-' + date[0]\n","                get_date = 1\n","            yyyy = re.search('\\d{4}', org, flags=0)\n","            if (yyyy != None and get_date == 0):\n","                yyyy = yyyy.group(0)\n","                org = org.replace(yyyy, '')\n","                nor = yyyy + '-'\n","            mm = re.search('January|February|March|April|May|June|July|August|September|October|November|December', org, flags=0)\n","            if (mm != None and get_date == 0):\n","                mm = mm.group(0)\n","                org = org.replace(mm, '')\n","                mm = mm.replace('January', '01')\n","                mm = mm.replace('Feburary', '02')\n","                mm = mm.replace('March', '03')\n","                mm = mm.replace('April', '04')\n","                mm = mm.replace('May', '05')\n","                mm = mm.replace('June', '06')\n","                mm = mm.replace('July', '07')\n","                mm = mm.replace('August', '08')\n","                mm = mm.replace('September', '09')\n","                mm = mm.replace('October', '10')\n","                mm = mm.replace('November', '11')\n","                mm = mm.replace('December', '12')\n","                nor = nor + mm + '-'\n","            dd = re.search('\\d{1,2}((st)|(nd)|(rd)|(th))', org, flags=0)\n","            if (dd != None and get_date == 0):\n","                dd = dd.group(0)\n","                org = org.replace(dd, '')\n","                dd = dd.replace('st', '')\n","                dd = dd.replace('nd', '')\n","                dd = dd.replace('rd', '')\n","                dd = dd.replace('th', '')\n","                if (len(dd) == 1):\n","                    dd = '0' + dd\n","                nor = nor + dd\n","            get_time = 0\n","            time = re.search('\\d{1,2}(:|\\.)\\d{1,2}', org, flags=0)\n","            if (time != None):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                time = re.split('\\.|:', time)\n","                if (pm == 1 and int(time[0]) < 12):\n","                    time[0] = str(int(time[0]) + 12)\n","                elif (am == 1 and int(time[0]) == 12):\n","                    time[0] = '00'\n","                if (len(time[0]) == 1):\n","                    time[0] = '0' + time[0]\n","                nor = nor + 'T' + time[0] + ':' + time[1]\n","                get_time = 1\n","            pm = 0\n","            am = 0\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            time = re.search('\\d{1,4}', org, flags=0)\n","            if (time != None and get_time == 0):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                hh, mm = '00', '00'\n","                if (len(time) == 4):\n","                    hh = time[0:2]\n","                    mm = time[2:]\n","                elif (len(time) == 3):\n","                    hh = time[0]\n","                    mm = time[1:]\n","                elif (len(time) == 2):\n","                    hh = time\n","                elif (len(time) == 1):\n","                    hh = time\n","                if (pm == 1 and int(hh) < 12):\n","                    hh = str(int(hh) + 12)\n","                elif (am == 1 and int(hh) == 12):\n","                    hh = '00'\n","                nor = nor + 'T' + hh + ':' + mm    \n","            #if (nor != ans):    \n","                #print(f'1:nor={nor}, ans={ans}, org={tmp}')\n","        elif (re.match('\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', org)):\n","            tmp = org\n","            nor = org.replace(' ', 'T')\n","            #if (nor != ans):    \n","                #print(f'2:nor={nor}, ans={ans}, org={tmp}')\n","        elif (re.match('(at |)(\\d{1,2}|)(:|\\.|)\\d{2}( |)(am|pm|Hr|Hrs|hr|hrs|)( on | )(the |)\\d{1,2}(\\/|\\.)\\d{2,4}(\\/|\\.)\\d{1,2}', org)):\n","            tmp = org\n","            pm = 0\n","            am = 0\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            date = re.search('\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org, flags=0)\n","            if (date != None):\n","                date = date.group(0)\n","                org = org.replace(date, '')\n","                date = re.split('\\/|\\.', date)\n","                if (len(date[0]) == 1):\n","                    date[0] = '0' + date[0]\n","                if (len(date[1]) == 1):\n","                    date[1] = '0' + date[1]\n","                if (len(date[2]) == 2):\n","                    date[2] = '20' + date[2]\n","                elif (len(date[2]) == 3):\n","                    date[2] = '2' + date[2]\n","                nor = date[2] + '-' + date[1] + '-' + date[0] + 'T'\n","            org = org.replace(':', '')\n","            time = re.search('\\d{1,4}', org, flags=0)\n","            if (time != None):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                hh, mm = '00', '00'\n","                if (len(time) == 4):\n","                    hh = time[0:2]\n","                    mm = time[2:]\n","                elif (len(time) == 3):\n","                    hh = time[0]\n","                    mm = time[1:]\n","                elif (len(time) == 2):\n","                    hh = time\n","                elif (len(time) == 1):\n","                    hh = time\n","                if (pm == 1 and int(hh) < 12):\n","                    hh = str(int(hh) + 12)\n","                elif (am == 1 and int(hh) == 12):\n","                    hh = '00'\n","                nor = nor + hh + ':' + mm\n","            #if (nor != ans):    \n","                #print(f'3:nor={nor}, ans={ans}, org={tmp}')\n","        elif (re.match('((\\d{1,2}((pm)|(am)))|(\\d{4}(Hr|Hrs|hr|hrs|)))(( on )| )\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org)):\n","            tmp = org\n","            pm = 0\n","            am = 0\n","            if (re.search('pm', org, flags=0) != None):\n","                pm = 1\n","            if (re.search('am', org, flags=0) != None):\n","                am = 1\n","            date = re.search('\\d{1,2}(\\/|\\.)\\d{1,2}(\\/|\\.)\\d{2,4}', org, flags=0)\n","            if (date != None):\n","                date = date.group(0)\n","                org = org.replace(date, '')\n","                date = re.split('\\/|\\.', date)\n","                if (len(date[0]) == 1):\n","                    date[0] = '0' + date[0]\n","                if (len(date[1]) == 1):\n","                    date[1] = '0' + date[1]\n","                if (len(date[2]) == 2):\n","                    date[2] = '20' + date[2]\n","                elif (len(date[2]) == 3):\n","                    date[2] = '2' + date[2]\n","                nor = date[2] + '-' + date[1] + '-' + date[0] + 'T'\n","            hrtime = re.search('\\d{4}', org, flags=0)\n","            if (hrtime != None):\n","                hrtime = hrtime.group(0)\n","                org = org.replace(hrtime, '')\n","                nor = nor + hrtime[0:2] + ':' + hrtime[2:]\n","            time = re.search('\\d{1,2}', org, flags=0)\n","            if (time != None):\n","                time = time.group(0)\n","                org = org.replace(time, '')\n","                hh = time\n","                if (pm == 1 and int(hh) < 12):\n","                    hh = str(int(hh) + 12)\n","                elif (am == 1 and int(hh) == 12):\n","                    hh = '00'\n","                if (len(hh) == 1):\n","                    hh = '0' + hh\n","                nor = nor + hh + ':' + '00'\n","            #if (nor != ans):    \n","                #print(f'4:nor={nor}, ans={ans}, org={tmp}')\n","    elif (time_type == 'DURATION'):   \n","        tmp = org\n","        org = org.replace('one', '1')\n","        org = org.replace('two', '2')\n","        org = org.replace('three', '3')\n","        org = org.replace('four', '4')\n","        org = org.replace('five', '5')\n","        num = ''\n","        alp = ''\n","        space_idx = org.find(' ')\n","        for i in range(len(org)):\n","            if (org[i] == 'D' or org[i] == 'd' or\\\n","                org[i] == 'W' or org[i] == 'w' or\\\n","                org[i] == 'M' or org[i] == 'm' or\\\n","                org[i] == 'Y' or org[i] == 'y') and i > space_idx:\n","                alp = org[i]\n","                org = org[:i]\n","                break\n","        # print(org, alp)\n","        org = re.split('-| ', org)\n","        try:\n","            if org[0].isalpha():\n","                org[0] = w2n.word_to_num(org[0])\n","            # print(org)\n","            if (len(org) == 1 or org[1] == ''):\n","                nor = 'P' + str(org[0]) + alp.upper()\n","            else:\n","                nor = 'P' + str((int(org[0]) + int(org[1])) / 2) + alp.upper()\n","        except:\n","            nor = tmp\n","        # if (nor != ans):    \n","        #     print(f'dur:nor={nor}, ans={ans}, org={tmp}')\n","    elif (time_type == 'SET'):\n","        if (re.match('twice', org)):\n","            nor = 'R2'\n","    return nor\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_ans_df = pd.DataFrame({\n","    'file_id': [],\n","    'PHI_type': [],\n","    'PHI_start': [],\n","    'PHI_end': [],\n","    'PHI_content': [],\n","    'ISO': []\n","})\n","\n","last_fid = \"\"\n","last_idx_of_last_seg = 0\n","for fid_sid, entities in result_ans_dict.items():\n","    curr_fid = fid_sid.split('_')[0]\n","    curr_sid = fid_sid.split('_')[1]\n","    # print(fid_sid)\n","\n","    if curr_fid != last_fid:\n","        with open(os.path.join('./opendid_test/opendid_test', curr_fid+'.txt'), 'r') as file:\n","            content = file.read()\n","        last_fid = curr_fid\n","        last_idx_of_last_seg = 0\n","\n","    # last_idx_of_last_seg = 0\n","\n","    for i, entity in enumerate(entities):\n","        new_row = []\n","        # print(i, entity)\n","\n","        if i == len(entities) - 1 and entity['entity_group'] == 'OTHER':\n","            last_idx_of_last_seg += len(result_segments[fid_sid])\n","            continue\n","        elif entity['entity_group'] != 'OTHER':\n","            # print(fid_sid)\n","            # print(last_idx_of_last_seg)\n","            start_idx = entity['start'] + last_idx_of_last_seg + int(curr_sid) - 1\n","            end_idx = entity['end'] + last_idx_of_last_seg + int(curr_sid) - 1\n","            # print('start', start_idx)\n","            # print(entity['word'])\n","            # find_idx = content.lower()[start_idx:].find(entity['word']) + start_idx\n","            # print('find', find_idx)\n","            # end_idx = find_idx+len(entity['word'])\n","            word = content[start_idx:end_idx]\n","\n","            if i == len(entities) - 1:\n","                last_idx_of_last_seg += len(result_segments[fid_sid])\n","\n","            if len(word) == 1 and word.isalnum() == False: \n","                continue\n","            if len(word) > 1:\n","                # print(word, start_idx, end_idx)\n","                while word[0].isalnum() == False or word[-1].isalnum() == False:\n","                    if word[0].isalnum() == False:\n","                        word = word[1:]\n","                        start_idx += 1\n","                    # print(word, start_idx, end_idx)\n","                    if word[-1].isalnum() == False and word[-1] == ')': \n","                        break\n","                    elif word[-1].isalnum() == False: # in case it is a newline character\n","                        word = word[:-1]\n","                        end_idx -= 1\n","            \n","            if '\\n' in word:\n","                word = word.replace('\\n', ' ') # in case contains newline character\n","\n","            label = entity['entity_group']\n","\n","            if entity['entity_group'] == 'DURATION' and word != 'twice':\n","                last_index = start_idx - 1\n","                # case 1: no spcae between number and month, week or year\n","                if content[last_index].isdigit():\n","                    while content[last_index].isdigit():\n","                        last_index -= 1\n","                    start_idx = last_index + 1\n","                    word = content[start_idx:end_idx]\n","                else:\n","                    last_space1 = content.rfind(' ', 0, start_idx)\n","                    last_space2 = content.rfind(' ', 0, last_space1)\n","                    start_idx = last_space2 + 1\n","                    word = content[start_idx:end_idx]\n","            elif word == 'twice':\n","                label = 'SET'\n","            \n","            new_row.extend([curr_fid, label, start_idx, end_idx, word])\n","            \n","            # print(curr_fid, label, start_idx, end_idx, word)\n","\n","            need_iso = ['DATE', 'TIME', 'DURATION', 'SET']\n","\n","            if label in need_iso:\n","                new_row.append(Normalize(label, word))\n","            else:\n","                new_row.append('')\n","            \n","            test_ans_df.loc[len(test_ans_df)] = new_row\n","        else:\n","            continue\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_ans_df"]},{"cell_type":"markdown","metadata":{},"source":["### Add in CRF predictions that PERT misses"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_crf = pd.read_csv('./opendid_test/crf_answer.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_crf_dur = df_crf.loc[df_crf['PHI_type'] == 'DURATION']\n","df_crf_set = df_crf.loc[df_crf['PHI_type'] == 'SET']\n","df_crf_lo = df_crf.loc[df_crf['PHI_type'] == 'LOCATION-OTHER'] "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_crf_lo"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_ans_df.loc[test_ans_df['file_id'] == '2465']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# New row data\n","crf_set_row_idx = 6905\n","new_row = df_crf.iloc[crf_set_row_idx]\n","\n","# Determine the position to insert the new row\n","insert_position = 8619\n","\n","# Create a new Series with the new row data\n","new_series = pd.Series(new_row)\n","\n","# Shift down the rows below the insertion point\n","test_ans_df = pd.concat([test_ans_df.iloc[:insert_position], new_series.to_frame().transpose(), test_ans_df.iloc[insert_position:]]).reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# New row data\n","crf_lo_row_idx = 3401\n","new_row = df_crf.iloc[crf_lo_row_idx]\n","\n","# Determine the position to insert the new row\n","insert_position = 4221\n","\n","# Create a new Series with the new row data\n","new_series = pd.Series(new_row)\n","\n","# Shift down the rows below the insertion point\n","test_ans_df = pd.concat([test_ans_df.iloc[:insert_position], new_series.to_frame().transpose(), test_ans_df.iloc[insert_position:]]).reset_index(drop=True)\n","\n","test_ans_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rows_to_delete = [4222, 4223, 4224]\n","\n","# Use the drop method to delete the specified rows\n","test_ans_df = test_ans_df.drop(rows_to_delete)\n","\n","test_ans_df.loc[test_ans_df['file_id'] == '2465']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_ans_df.to_csv('./opendid_test/test_answer.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_ans_df.to_csv('./opendid_test/test_answer.txt', sep='\\t', header=False, index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
